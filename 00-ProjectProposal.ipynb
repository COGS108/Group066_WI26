{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Project Proposal\n",
    "\n",
    "## Authors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Brian Liu: Conceptualization, Analysis, Writing – original draft\n",
    "- Anchita Dash:  Data curation,  Experimental investigation, Writing – original draft\n",
    "- Zihan Zhang:  Project administration, Visualization, Writing – original draft\n",
    "- Ariane Hai: Background research, Methodology, Writing – original draft"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using statistical inference, what are the top predictive features among quantitative game statistics—such as total ratings, install milestones, and growth rates—that determine a game's specific rank within the Google Play Store’s genre-specific top 100 charts? Furthermore, does the relative importance of these features vary significantly across different game genres? \n",
    "\n",
    "**Target variable**   \n",
    "- **rank** (ordinal/numerical: the game’s rank (1–100) in the genre’s top 100 list) \n",
    "\n",
    "**Predictor variables** \n",
    "- **total ratings** (numerical: game’s total number of ratings) \n",
    "- **installs** (ordinal/numerical: game’s approximate install milestone—e.g., 100.0 M installs vs. 500.0 M installs) \n",
    "- **average rating** (numerical: average rating out of 5)  \n",
    "- **growth (30 days)** (numerical: percent growth in 30 days)  \n",
    "- **growth (60 days)** (numerical: percent growth in 60 days) \n",
    "- **price** (numerical: price in dollars)   \n",
    "- **5 star ratings** (numerical: number of 5 star ratings) \n",
    "\n",
    "**Grouping variable**\n",
    "- **category** (nominal: genre of the game) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Google Play Store is the top distributor of downloadable content for Android devices. Run by Google as Android's official app store, it offers apps, games, books, movies, TV, and other media<sup><a href=\"#ref1\">1</a></sup>. As of 2024, the Google Play Store offered approximately 264,000 mobile games ranging across genres like Action, Puzzle, and RPG<sup><a href=\"#ref2\">2</a></sup>.\n",
    "\n",
    "Within this ecosystem, top chart rankings play an important role in influencing which games users actually see and download. Higher ranked games receive more visibility in lists, search results, and curated sections, which in turn increases the likelihood that users will click through and install them<sup><a href=\"#ref3\">3</a></sup>. However, since Google does not disclose its full algorithm behind rankings, it remains unclear which measurable performance metrics, such as install volume, rating averages, or review counts, most strongly predict a game's chart position<sup><a href=\"#ref3\">3</a></sup>.\n",
    "\n",
    "From our own experience scrolling through the Play Store, we noticed that we almost always check the star rating and number of reviews before downloading a game. If an app has a high average rating and thousands of reviews, it just feels more trustworthy than one with barely any feedback. So, we hypothesize that review count and average star rating will be the features most strongly correlated with a game's ranking.\n",
    "\n",
    "With hundreds of thousands of games competing for attention on the Play Store, pinpointing the very factors that increase the attraction of games will help game developers to focus their resources on improving in those specific areas instead of guessing what might work. This gap motivates our analysis: We seek to identify which quantitative signals in the Play Store data correlate most closely with ranking, thereby shedding light on the factors that likely drive discoverability in this marketplace. The \"Top Games on Google Play Store\" dataset provides rankings for the top 100 games across multiple genres, allowing us to analyze whether the relative importance of these predictive factors varies significantly between categories like Action, Puzzle, and RPG.\n",
    "\n",
    "Prior studies indicate that publishers' prior release volume, intragenre ranking, consumer ratings, and review volume jointly drive high levels of game downloads, with the influence of any single information cue varying depending on the presence of other coexisting cues<sup><a href=\"#ref4\">4</a></sup>. Complementary research using TAM and IDT based feature selection methods further shows that mobile game downloads are shaped by both usability perceptions, such as perceived ease of use and usefulness, and diffusion-related factors, including visibility, trialability, and social communication channels<sup><a href=\"#ref5\">5</a></sup>. Together, these findings suggest that download performance emerges from interacting quantitative and contextual signals, a premise that directly informs our investigation of how such signals translate into genre specific ranking outcomes on the Google Play Store and similar app distribution platforms.\n",
    "\n",
    "From the Google Play Store project<sup><a href=\"#ref6\">6</a></sup>, something that we could incorporate into our project are the different graphs that are present. The bar plot that outlines the number of paid and free apps based on category is very useful to see if the category in particular affects the ranking. The project also does a great job of looking at how ratings differ between paid and free apps. This is something that we could also incorporate into our project and see how pricing affects ranking. The correlation matrix is also useful since we are trying to figure out which features strongly influence rankings and this matrix summarizes it well by finding the correlation between ranking and other features.\n",
    "\n",
    "From the app store project<sup><a href=\"#ref7\">7</a></sup>, something that we could incorporate into our project is making a pipeline since that is nice especially if we want new unclean and untidy data for prediction to go through the same processes that we undertook for data cleaning/preprocessing in our project since that would make sure that the new data is fit for modeling and prediction. There were different ML models used like linear regression, SVM regression, polynomial regression etc. and their RMSE were compared, this is also nice if we want to see how each model performs on the validation set and use the best one later for the test set.\n",
    "\n",
    "**References**\n",
    "\n",
    "<a name=\"ref1\"></a> ^ UW Connect. What is Google Play Store. [https://uwconnect.uw.edu/it?id=kb_article_view&sysparm_article=KB0034369](https://uwconnect.uw.edu/it?id=kb_article_view&sysparm_article=KB0034369)\n",
    "\n",
    "<a name=\"ref2\"></a> ^ Statista. (2024). Number of available gaming apps in the Google Play Store. [https://www.statista.com/statistics/780229/number-of-available-gaming-apps-in-the-google-play-store-quarter/](https://www.statista.com/statistics/780229/number-of-available-gaming-apps-in-the-google-play-store-quarter/)\n",
    "\n",
    "<a name=\"ref3\"></a> ^ Appinventiv. How rankings are determined in Google Play Store. [https://appinventiv.com/blog/google-play-store-statistics/](https://appinventiv.com/blog/google-play-store-statistics/)\n",
    "\n",
    "<a name=\"ref4\"></a> ^ Emerald Insight. (2022). The influence of information configuration on mobile game download. [https://www.emerald.com/intr/article-abstract/32/4/1191/176295/The-influence-of-information-configuration-on](https://www.emerald.com/intr/article-abstract/32/4/1191/176295/The-influence-of-information-configuration-on)\n",
    "\n",
    "<a name=\"ref5\"></a> ^ Springer. A Study of Downloading Game Applications. [https://link.springer.com/chapter/10.1007/978-3-662-47200-2_90](https://link.springer.com/chapter/10.1007/978-3-662-47200-2_90)\n",
    "<a name=\"ref6\"></a> ^ Alhajali, A. N. Google Play Store App Dataset Analysis. Kaggle. [https://www.kaggle.com/code/ammarnassanalhajali/google-play-store-app-dataset-analysis/notebook](https://www.kaggle.com/code/ammarnassanalhajali/google-play-store-app-dataset-analysis/notebook)\n",
    "<a name=\"ref7\"></a> ^ Nish, A. Analysis of Apple's App Store. Kaggle. [https://www.kaggle.com/code/avnishnish/analysis-of-apple-s-app-store](https://www.kaggle.com/code/avnishnish/analysis-of-apple-s-app-store)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesize that total ratings and average rating will be the most statistically significant predictors of chart ranking. Intuitively, these features seem the most pertinent factors for assessing rank. We also anticipate that the relative weight of these features will vary by genre, with total ratings dominating mass-appeal categories like Action, while average rating holds greater predictive power in strategy-focused genres where quality drives retention."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal Dataset\n",
    "\n",
    "To fully answer our research question about which quantitative features predict genre-specific game rankings, the ideal dataset would contain the following variables for each game: rank (ordinal 1–100 position within its genre's top chart), category/genre (nominal), total ratings (numerical count of all user ratings), average rating (numerical 1–5 stars), install milestone (ordinal categorical such as \"100,000+\" or \"10,000,000+\"), price (numerical in USD), and growth metrics (numerical percentage change over 30 and 60 days). Ideally, we would also capture 5-star rating counts to analyze rating distribution skew.\n",
    "\n",
    "We would need at least 10,000 observations to ensure sufficient statistical power for detecting meaningful correlations and for conducting reliable train-test splits during modeling. This sample size would allow us to analyze multiple genres separately while maintaining adequate observations per category.\n",
    "\n",
    "These data would be collected via web scraping of the official Google Play Store API or through Google's BigQuery public datasets, capturing a time-stamped snapshot of all games appearing in genre-specific top 100 charts. The data would be stored in a structured CSV, organized with clear column headers and data types (numeric, categorical, ordinal) to facilitate analysis in Python using pandas.\n",
    "\n",
    "### Real Datasets\n",
    "\n",
    "**Top Games on Google Play Store** (https://www.kaggle.com/datasets/dhruvildave/top-play-store-games)\n",
    "This Kaggle dataset contains the top 100 games for each category on the Google Play Store, including variables for rank, category, total ratings, average rating, install milestones, price, growth (30 days), growth (60 days), and 5-star ratings.\n",
    "\n",
    "**Google Play Store Reviews** (https://www.kaggle.com/datasets/prakharrathi25/google-play-store-reviews)\n",
    "This dataset contains over 12,000 user reviews with associated star ratings and sentiment labels. While our primary analysis focuses on aggregate metrics rather than individual reviews, this dataset could allow us to explore whether review sentiment, review volume, app name, and overall rating correlates with ranking.\n",
    "\n",
    "**Google Play Store Apps** (https://www.kaggle.com/datasets/lava18/google-play-store-apps)\n",
    "Although this dataset does not include the specific rank variable we need, the category, installs, and rating columns overlap with our predictors of interest, allowing us to compare patterns across the full app catalog versus just the top-ranked games.\n",
    "\n",
    "### Differences Between Ideal and Real Data\n",
    "\n",
    "Our ideal dataset would include precise numerical install counts rather than ordinal milestones (e.g., exact \"1,247,583 installs\" vs. \"1,000,000+\"), would capture daily or weekly ranking fluctuations rather than a single snapshot, and would span a longer time period to enable temporal analysis. The real datasets provide ordinal install ranges and a single time-point snapshot, which limits our ability to track ranking dynamics over time. However, the Kaggle \"Top Games on Google Play Store\" dataset comes closest to our ideal by providing exact rank positions and all key quantitative metrics we hypothesize are predictive, making it sufficient for conducting correlation analysis and building predictive models to answer our research question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ethics \n",
    "\n",
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    "\n",
    "> Our dataset only includes the top 100 ranked games per genre, which introduces significant selection bias. By excluding games ranked below 100, we systematically omit indie games, newer releases, and potentially diverse developers who haven't broken into the top charts. This could skew our findings toward characteristics of already-successful games with substantial marketing budgets, potentially reinforcing the advantage of established publishers. We acknowledge this limitation and will interpret our results as applicable to high-ranking games specifically, rather than generalizing to the entire Play Store ecosystem.\n",
    "\n",
    " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    " - [X] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    " - [X] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    "\n",
    "> Our analysis focuses on quantitative metrics visible to users (ratings, installs, growth), but we recognize we lack perspective from game developers who understand the internal factors affecting rankings. Additionally, we do not account for qualitative factors like gameplay innovation, artistic merit, or accessibility features that may influence user satisfaction but not be captured in star ratings. To partially address this, we reviewed existing literature on mobile game success factors and will acknowledge that our statistical model captures correlation, not necessarily the causal mechanisms developers could control.\n",
    "\n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "\n",
    "### D. Modeling\n",
    " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    " - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    " - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "\n",
    "### E. Deployment\n",
    " - [X] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    " - [X] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    " - [X] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will use Discord as our primary means of communication. Responses should be within 24 hours. We will meet every Wednesday at Geisel at 5 PM unless notified otherwise through Discord.\n",
    "\n",
    "- We will communicate in a supportive tone with each other. Every message should be responded to. If there is no response needed, always like the message. We will be inclusive of all opinions; no opinion should be rejected without being heard. To express disagreement, we will start with “I personally think.”\n",
    "\n",
    "- Decisions should be made with the consent of all group members. Unless there are significant conflicts, the general rule of thumb should apply. If an immediate decision is needed, at least one other member must agree with the person proposing the decision.\n",
    "\n",
    "- Everyone is expected to do a little bit of everything. Since we all have different strengths, members should be proactive in their area of expertise. Always reach out for help if there is difficulty. Communicate openly if work needs to be split differently.\n",
    "\n",
    "- If a deadline cannot be met, notify the group as soon as possible (at least 36 hours before the deadline). The team will come together to figure out a solution. We commit to being understanding and supportive, with no pressure in communicating these situations. Each member has two chances to request deadline flexibility.\n",
    "\n",
    "- For issues such as problem teammates or conflicts, communicate with the entire group first. The group will decide together whether the issue should be taken outside the group.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date | Meeting Time | Completed Before Meeting | Discussed At Meeting |\n",
    "|-------------|-------------|--------------------------|----------------------|\n",
    "| 1/21 | 5:00 PM | Reviewed project description and brainstormed initial project ideas | Reviewed project requirements, discussed potential research questions, explored possible datasets, and determined regular meeting times |\n",
    "| 1/28 | 5:00 PM | Conducted background research related to the project topic | Completed project review and refined understanding of project expectations |\n",
    "| 2/3 | 3:20 PM | Reviewed draft project proposal and brainstormed more specific research questions | Began drafting the project proposal, identified and selected datasets, and finalized the research question |\n",
    "| 2/4 | 5:00 PM | Explored and familiarized ourselves with the datasets relevant to the research question | Finished Project Proposal |\n",
    "| 2/11 | 5:00 PM | Reviewed and understood Checkpoint #1 requirements | Began Checkpoint #1, assigned tasks to group members, started dataset preprocessing and initial coding, and discussed wrangling and analytical approaches |\n",
    "| 2/18 | 5:00 PM | Group members completed assigned portions of Checkpoint #1 | Reviewed and finalized Checkpoint #1 |\n",
    "| 2/25 | 5:00 PM | Reviewed EDA and Checkpoint #2 requirements and completed exploratory data analysis using the fully processed dataset | Assigned tasks, loaded cleaned datasets, conducted EDA, and discussed observed patterns and insights |\n",
    "| 3/4 | 5:00 PM | Reviewed final video requirements and brainstormed presentation ideas | Finalized Checkpoint #2, discussed next stages of analysis, planned the final video structure, drafted scripts, and began filming |\n",
    "| 3/11 | 5:00 PM | Filmed individual segments and reviewed recorded clips | Completed filming, re-filmed unsatisfactory clips, and began video editing |\n",
    "| 3/17 | 5:00 PM | Continued editing the final video | Verified all requirements were met and finalized the project |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
